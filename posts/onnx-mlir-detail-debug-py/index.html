<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta http-equiv=x-ua-compatible content="IE=edge, chrome=1"><title>ONNX MLIRのdebug.py詳細 - Keep Coding, Keep Climbing</title><meta name=Description content="programming and rock climbing"><meta property="og:title" content="ONNX MLIRのdebug.py詳細"><meta property="og:description" content="ONNX MLIRに付属のdebug.pyを動かすで動かしたdebug.pyの中を見る。 debug.pyの処理内容 概要 入力として指定したONNXモデ"><meta property="og:type" content="article"><meta property="og:url" content="https://keepcodingkeepclimbing.com/posts/onnx-mlir-detail-debug-py/"><meta property="og:image" content="https://keepcodingkeepclimbing.com/img/avatar.jpg"><meta property="article:published_time" content="2020-08-05T00:00:00+00:00"><meta property="article:modified_time" content="2020-08-05T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://keepcodingkeepclimbing.com/img/avatar.jpg"><meta name=twitter:title content="ONNX MLIRのdebug.py詳細"><meta name=twitter:description content="ONNX MLIRに付属のdebug.pyを動かすで動かしたdebug.pyの中を見る。 debug.pyの処理内容 概要 入力として指定したONNXモデ"><meta name=application-name content="Keep Coding, Keep Climbing"><meta name=apple-mobile-web-app-title content="Keep Coding, Keep Climbing"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://keepcodingkeepclimbing.com/posts/onnx-mlir-detail-debug-py/><link rel=prev href=https://keepcodingkeepclimbing.com/posts/onnx-mlir-debug-py/><link rel=next href=https://keepcodingkeepclimbing.com/posts/run-onnx-mlir-shared-library/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"ONNX MLIRのdebug.py詳細","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/keepcodingkeepclimbing.com\/posts\/onnx-mlir-detail-debug-py\/"},"genre":"posts","keywords":"ONNX, MLIR, Python","wordcount":1328,"url":"https:\/\/keepcodingkeepclimbing.com\/posts\/onnx-mlir-detail-debug-py\/","datePublished":"2020-08-05T00:00:00+00:00","dateModified":"2020-08-05T00:00:00+00:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"Masahiro Hiramori"},"author":{"@type":"Person","name":"     \"Masahiro Hiramori\""},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':('auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark'))&&document.body.setAttribute('theme','dark');</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Keep Coding, Keep Climbing">Keep Coding, Keep Climbing</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/cv>CV </a><a class=menu-item href=/about>About </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Keep Coding, Keep Climbing">Keep Coding, Keep Climbing</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/>Posts</a><a class=menu-item href=/tags/>Tags</a><a class=menu-item href=/categories/>Categories</a><a class=menu-item href=/cv>CV</a><a class=menu-item href=/about>About</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animated flipInX">ONNX MLIRのdebug.py詳細</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw"></i>    "Masahiro Hiramori"</a></span>&nbsp;<span class=post-category>included in <a href=/categories/tech/><i class="far fa-folder fa-fw"></i>Tech</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2020-08-05>2020-08-05</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;1328 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;3 minutes&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#debugpyの処理内容><code>debug.py</code>の処理内容</a><ul><li><a href=#概要>概要</a></li><li><a href=#詳細>詳細</a></li></ul></li></ul></nav></div></div><div class=content id=content><p><a href=https://keepcodingkeepclimbing.com/posts/onnx-mlir-debug-py/ rel>ONNX MLIRに付属のdebug.pyを動かす</a>で動かした<code>debug.py</code>の中を見る。</p><h2 id=debugpyの処理内容><code>debug.py</code>の処理内容</h2><h3 id=概要>概要</h3><ul><li>入力として指定したONNXモデルを、ONNX MLIRでshared libraryとしてビルド、実行し、リファレンスバックエンドで実行した結果と比較する</li><li>リファレンスバックエンドにはONNX Runtimeを使用</li><li>Operatorのoutputごとに比較<ul><li>モデルの出力だけでなく、Operatorの実行結果単位で比較している</li></ul></li><li><code>PyRuntime</code>はおそらく、ONNX MLIRでビルドしたshared libraryを、Pythonから実行するためのPythonバインディング<ul><li>shared libraryの実行方法は<code>PyRuntime</code>の実装を見る必要がありそう</li></ul></li></ul><h3 id=詳細>詳細</h3><p><code>import onnxruntime</code>でONNX Runtimeをインポートする。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>os</span>
<span class=kn>import</span> <span class=nn>sys</span>
<span class=kn>import</span> <span class=nn>argparse</span>
<span class=kn>import</span> <span class=nn>onnx</span>
<span class=kn>import</span> <span class=nn>subprocess</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=kn>as</span> <span class=nn>np</span>
<span class=kn>import</span> <span class=nn>tempfile</span>

<span class=kn>from</span> <span class=nn>collections</span> <span class=kn>import</span> <span class=n>OrderedDict</span>

<span class=c1># Reference backend, use onnxruntime by default</span>
<span class=kn>import</span> <span class=nn>onnxruntime</span>
<span class=n>prepare</span> <span class=o>=</span> <span class=n>onnxruntime</span><span class=o>.</span><span class=n>InferenceSession</span>
</code></pre></td></tr></table></div></div><p><code>ONNX_MLIR_HOME</code>が設定されているか確認。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=k>if</span> <span class=p>(</span><span class=ow>not</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;ONNX_MLIR_HOME&#39;</span><span class=p>,</span> <span class=bp>None</span><span class=p>)):</span>
    <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span>
        <span class=s2>&#34;Environment variable ONNX_MLIR_HOME is not set, please set it to the path to &#34;</span>
        <span class=s2>&#34;the HOME directory for onnx-mlir. The HOME directory for onnx-mlir refers to &#34;</span>
        <span class=s2>&#34;the parent folder containing the bin, lib, etc sub-folders in which ONNX-MLIR &#34;</span>
        <span class=s2>&#34;executables and libraries can be found.&#34;</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p><code>onnx-mlir</code>実行ファイルパス、<code>lib</code>ディレクトリをimport検索パスに追加などする。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=n>VERBOSE</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;VERBOSE&#39;</span><span class=p>,</span> <span class=bp>False</span><span class=p>)</span>
<span class=n>ONNX_MLIR</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;ONNX_MLIR_HOME&#39;</span><span class=p>],</span> <span class=s2>&#34;bin/onnx-mlir&#34;</span><span class=p>)</span>

<span class=c1># Include runtime directory in python paths, so PyRuntime can be imported.</span>
<span class=n>RUNTIME_DIR</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;ONNX_MLIR_HOME&#39;</span><span class=p>],</span> <span class=s2>&#34;lib&#34;</span><span class=p>)</span>
<span class=n>sys</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>RUNTIME_DIR</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p><code>PyRuntime</code>(ONNX MLIRでビルドしたshared libraryをPythonから実行するためのPythonバインディング？)をインポートする。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=k>try</span><span class=p>:</span>
    <span class=kn>from</span> <span class=nn>PyRuntime</span> <span class=kn>import</span> <span class=n>ExecutionSession</span>
<span class=k>except</span> <span class=ne>ImportError</span><span class=p>:</span>
    <span class=k>raise</span> <span class=ne>ImportError</span><span class=p>(</span>
        <span class=s2>&#34;Looks like you did not build the PyRuntime target, build it by running `make PyRuntime`.&#34;</span>
    <span class=p>)</span>
</code></pre></td></tr></table></div></div><p>ユーティリティ関数を定義。</p><p><code>extend_model_output</code>関数は、モデル内の各Operatorのoutputに、data type、shape情報を追加する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>execute_commands</span><span class=p>(</span><span class=n>cmds</span><span class=p>):</span>
    <span class=k>if</span> <span class=p>(</span><span class=n>VERBOSE</span><span class=p>):</span>
        <span class=k>print</span><span class=p>(</span><span class=s2>&#34; &#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>cmds</span><span class=p>))</span>
    <span class=n>subprocess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>cmds</span><span class=p>,</span> <span class=n>stdout</span><span class=o>=</span><span class=n>subprocess</span><span class=o>.</span><span class=n>PIPE</span><span class=p>,</span> <span class=n>check</span><span class=o>=</span><span class=bp>True</span><span class=p>)</span>


<span class=k>def</span> <span class=nf>extend_model_output</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>intermediate_outputs</span><span class=p>):</span>
    <span class=c1># onnx-mlir doesn&#39;t care about manually specified output types &amp; shapes.</span>
    <span class=n>DUMMY_TENSOR_TYPE</span> <span class=o>=</span> <span class=n>onnx</span><span class=o>.</span><span class=n>TensorProto</span><span class=o>.</span><span class=n>FLOAT</span>

    <span class=k>while</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>output</span><span class=p>)):</span>
        <span class=n>model</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>output</span><span class=o>.</span><span class=n>pop</span><span class=p>()</span>

    <span class=k>for</span> <span class=n>output_name</span> <span class=ow>in</span> <span class=n>intermediate_outputs</span><span class=p>:</span>
        <span class=n>output_value_info</span> <span class=o>=</span> <span class=n>onnx</span><span class=o>.</span><span class=n>helper</span><span class=o>.</span><span class=n>make_tensor_value_info</span><span class=p>(</span>
            <span class=n>output_name</span><span class=p>,</span> <span class=n>DUMMY_TENSOR_TYPE</span><span class=p>,</span> <span class=bp>None</span><span class=p>)</span>
        <span class=n>model</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>output</span><span class=o>.</span><span class=n>extend</span><span class=p>([</span><span class=n>output_value_info</span><span class=p>])</span>
    <span class=k>return</span> <span class=n>model</span>
</code></pre></td></tr></table></div></div><p>メイン関数。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>main</span><span class=p>(</span><span class=n>model_path</span><span class=p>):</span>
</code></pre></td></tr></table></div></div><p>入力のONNXファイルを読み込み、各Operatorのoutputにdata type、shape情報を追加する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>    <span class=n>model</span> <span class=o>=</span> <span class=n>onnx</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>model_path</span><span class=p>)</span>
    <span class=n>intermediate_outputs</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span>
        <span class=p>[</span><span class=nb>list</span><span class=p>(</span><span class=n>node</span><span class=o>.</span><span class=n>output</span><span class=p>)</span> <span class=k>for</span> <span class=n>node</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>node</span><span class=p>],</span> <span class=p>[])</span>
    <span class=n>intermediate_outputs</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>OrderedDict</span><span class=o>.</span><span class=n>fromkeys</span><span class=p>(</span><span class=n>intermediate_outputs</span><span class=p>))</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>extend_model_output</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>intermediate_outputs</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p>以下は一時ディレクトリ内で実行。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>    <span class=k>with</span> <span class=n>tempfile</span><span class=o>.</span><span class=n>TemporaryDirectory</span><span class=p>()</span> <span class=k>as</span> <span class=n>temp_dir</span><span class=p>:</span>
        <span class=k>print</span><span class=p>(</span><span class=s2>&#34;Temporary directory has been created at {}&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>temp_dir</span><span class=p>))</span>
</code></pre></td></tr></table></div></div><p>ONNXモデルをファイルに出力し、ONNX MLIRでshared libraryに変換する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>        <span class=c1># Save modified model &amp; invoke onnx-mlir to compile it.</span>
        <span class=n>temp_model_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>temp_dir</span><span class=p>,</span> <span class=s2>&#34;model.onnx&#34;</span><span class=p>)</span>
        <span class=n>onnx</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>temp_model_path</span><span class=p>)</span>
        <span class=n>execute_commands</span><span class=p>([</span><span class=n>ONNX_MLIR</span><span class=p>,</span> <span class=n>temp_model_path</span><span class=p>])</span>
</code></pre></td></tr></table></div></div><p>shared libraryから実行セッションを作成する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>        <span class=c1># Use the generated shared library to create an execution session.</span>
        <span class=n>temp_shared_lib_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>temp_dir</span><span class=p>,</span> <span class=s2>&#34;model.so&#34;</span><span class=p>)</span>
        <span class=n>sess</span> <span class=o>=</span> <span class=n>ExecutionSession</span><span class=p>(</span><span class=n>temp_shared_lib_path</span><span class=p>,</span>
                                <span class=s2>&#34;_dyn_entry_point_main_graph&#34;</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p>入力データとしてランダムデータを生成する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>        <span class=c1># Generate random data as input.</span>
        <span class=n>inputs</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=n>input_names</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=n>initializers</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=nb>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=o>.</span><span class=n>name</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>initializer</span><span class=p>))</span>
        <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
        <span class=k>for</span> <span class=n>input_proto</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>input</span><span class=p>:</span>
            <span class=k>if</span> <span class=n>input_proto</span><span class=o>.</span><span class=n>name</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>initializers</span><span class=p>:</span>
                <span class=n>input_names</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>input_proto</span><span class=o>.</span><span class=n>name</span><span class=p>)</span>
                <span class=n>shape_proto</span> <span class=o>=</span> <span class=n>input_proto</span><span class=o>.</span><span class=n>type</span><span class=o>.</span><span class=n>tensor_type</span><span class=o>.</span><span class=n>shape</span>
                <span class=n>explicit_shape</span> <span class=o>=</span> <span class=p>[]</span>
                <span class=k>for</span> <span class=n>dim</span> <span class=ow>in</span> <span class=n>shape_proto</span><span class=o>.</span><span class=n>dim</span><span class=p>:</span>
                    <span class=k>assert</span> <span class=n>dim</span><span class=o>.</span><span class=n>dim_value</span><span class=p>,</span> <span class=s2>&#34;Can only debug models with inputs that have explicit shapes.&#34;</span>
                    <span class=n>explicit_shape</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>dim</span><span class=o>.</span><span class=n>dim_value</span><span class=p>)</span>
                <span class=n>inputs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
                    <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=o>-</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=n>explicit_shape</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>))</span>
</code></pre></td></tr></table></div></div><p>shared libraryの実行セッションを実行する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>        <span class=c1># Run the compiled inference function on the randomly generated data.</span>
        <span class=n>outs</span> <span class=o>=</span> <span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p>リファレンスバックエンド(ONNX Runtime)で実行する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>        <span class=c1># Run the model with reference backend and get results.</span>
        <span class=n>ref_session</span> <span class=o>=</span> <span class=n>prepare</span><span class=p>(</span><span class=n>temp_model_path</span><span class=p>)</span>
        <span class=n>output_names</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=nb>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=o>.</span><span class=n>name</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>output</span><span class=p>))</span>
        <span class=n>input_feed</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>input_names</span><span class=p>,</span> <span class=n>inputs</span><span class=p>))</span>
        <span class=n>ref_outs</span> <span class=o>=</span> <span class=n>ref_session</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>output_names</span><span class=p>,</span> <span class=n>input_feed</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p>shared libraryとONNX Runtimeで実行した結果を比較する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>        <span class=c1># For each intermediate output tensor, compare results.</span>
        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>name</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>intermediate_outputs</span><span class=p>):</span>
            <span class=k>print</span><span class=p>(</span><span class=s2>&#34;Verifying value of {}&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>name</span><span class=p>))</span>
            <span class=n>np</span><span class=o>.</span><span class=n>testing</span><span class=o>.</span><span class=n>assert_array_almost_equal</span><span class=p>(</span><span class=n>ref_outs</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>outs</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>decimal</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p>引数パーサーを定義し、メイン関数へパース結果を渡す。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
    <span class=n>parser</span> <span class=o>=</span> <span class=n>argparse</span><span class=o>.</span><span class=n>ArgumentParser</span><span class=p>()</span>
    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s1>&#39;model_path&#39;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>str</span><span class=p>,</span> <span class=n>help</span><span class=o>=</span><span class=s2>&#34;Path to the model to debug.&#34;</span><span class=p>)</span>
    <span class=n>args</span> <span class=o>=</span> <span class=n>parser</span><span class=o>.</span><span class=n>parse_args</span><span class=p>()</span>
    <span class=n>main</span><span class=p>(</span><span class=o>**</span><span class=nb>vars</span><span class=p>(</span><span class=n>args</span><span class=p>))</span>
</code></pre></td></tr></table></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2020-08-05</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/posts/onnx-mlir-detail-debug-py/index.md target=_blank>Read Markdown</a></span></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/onnx/>ONNX</a>,&nbsp;<a href=/tags/mlir/>MLIR</a>,&nbsp;<a href=/tags/python/>Python</a></section><section><span><a href=javascript:void(0); onclick=window.history.back();>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/onnx-mlir-debug-py/ class=prev rel=prev title="ONNX MLIRに付属のdebug.pyを動かす"><i class="fas fa-angle-left fa-fw"></i>ONNX MLIRに付属のdebug.pyを動かす</a>
<a href=/posts/run-onnx-mlir-shared-library/ class=next rel=next title="ONNX MLIRで出力したshared libraryを実行する">ONNX MLIRで出力したshared libraryを実行する<i class="fas fa-angle-right fa-fw"></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.74.1">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i>LoveIt</a></div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2018 - 2020</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>Masahiro Hiramori</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i></a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><link rel=stylesheet href=/custom-font.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.2.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js></script><script type=text/javascript>window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{}};</script><script type=text/javascript src=/js/theme.min.js></script></body></html>