<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>ONNX MLIRのdebug.py詳細 |</title><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content><meta name=twitter:title content="ONNX MLIRのdebug.py詳細"><meta name=twitter:description content><meta name=twitter:creator content="@mshrh3"><meta name=Description content="programming and rock climbing"><meta name=Hatena::Bookmark content="nocomment"><meta property="og:title" content="ONNX MLIRのdebug.py詳細"><meta property="og:description" content="ONNX MLIRに付属のdebug.pyを動かすで動かしたdebug.pyの中を見る。 debug.pyの処理内容 概要 入力として指定したONNXモデ"><meta property="og:type" content="article"><meta property="og:url" content="https://mshr-h.com/posts/onnx-mlir-detail-debug-py/"><meta property="og:image" content="https://mshr-h.com/img/avatar.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-08-05T00:00:00+00:00"><meta property="article:modified_time" content="2020-04-28T19:40:35+09:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://mshr-h.com/img/avatar.jpg"><meta name=twitter:title content="ONNX MLIRのdebug.py詳細"><meta name=twitter:description content="ONNX MLIRに付属のdebug.pyを動かすで動かしたdebug.pyの中を見る。 debug.pyの処理内容 概要 入力として指定したONNXモデ"><meta name=application-name content="Keep Coding, Keep Climbing"><meta name=apple-mobile-web-app-title content="Keep Coding, Keep Climbing"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=icon type=image/png sizes=192x192 href=/android-chrome-192x192.png><link rel=icon type=image/png sizes=512x512 href=/android-chrome-512x512.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://mshr-h.com/posts/onnx-mlir-detail-debug-py/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"ONNX MLIRのdebug.py詳細","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/mshr-h.com\/posts\/onnx-mlir-detail-debug-py\/"},"genre":"posts","keywords":"ONNX, MLIR, Python","wordCount":1328,"url":"https:\/\/mshr-h.com\/posts\/onnx-mlir-detail-debug-py\/","datePublished":"2020-08-05T00:00:00+00:00","dateModified":"2020-04-28T19:40:35+09:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Person","name":"","image":[{"@type":"ImageObject","url":"\/img\/avatar.jpg"}]},"description":""}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https:\/\/mshr-h.com"},{"@type":"ListItem","position":2,"name":"Tech","item":"https://mshr-h.com/categories/tech/"},{"@type":"ListItem","position":3,"name":"ONNX MLIRのdebug.py詳細"}]}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':'auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark')&&document.body.setAttribute('theme','dark')</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Keep Coding, Keep Climbing" class="header-logo logo-svg">Keep Coding, Keep Climbing</a></div><div class=menu><nav><ul class=menu-inner><li><a class=menu-item href=/posts/>Posts</a></li><li><a class=menu-item href=/tags/>Tags</a></li><li><a class=menu-item href=/categories/>Categories</a></li><li><a class=menu-item href=/cv>CV</a></li><li><a class=menu-item href=/about>About</a></li><li><a class=menu-item href=https://github.com/mshr-h title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i></a></li></ul></nav><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><span class="svg-icon icon-moon"></span></a></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Keep Coding, Keep Climbing" class=header-logo>Keep Coding, Keep Climbing</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><nav><ul><li><a class=menu-item href=/posts/ title>Posts</a></li><li><a class=menu-item href=/tags/ title>Tags</a></li><li><a class=menu-item href=/categories/ title>Categories</a></li><li><a class=menu-item href=/cv title>CV</a></li><li><a class=menu-item href=/about title>About</a></li><li><a class=menu-item href=https://github.com/mshr-h title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i></a></li></ul></nav><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><span class="svg-icon icon-moon"></span></a></div></div></header><main class=main><div class="container content-article page-toc theme-classic"><div class=toc id=toc-auto><div class=toc-title>Contents</div><div class=toc-content id=toc-content-auto></div></div><div class=header-post><div class=post-title><div class=post-all-meta><nav class=breadcrumbs><ol><li><a href=/>Home</a></li><li><a href=/categories/tech/>Tech</a></li><li>ONNX MLIRのdebug.py詳細</li></ol></nav><h1 class="single-title flipInX">ONNX MLIRのdebug.py詳細</h1><div class=post-meta><div class=post-meta-line><span class=post-category><a href=/categories/tech/><i class="svg-icon icon-folder"></i>Tech</a>
</span><span class=post-meta-date><i class="svg-icon icon-clock"></i><time class=timeago datetime=2020-08-05>2020-08-05</time>
</span><span class=post-meta-words><i class="svg-icon icon-pencil"></i>1328 words</span>
<span class=post-meta-reading><i class="svg-icon icon-stopwatch"></i>3 minutes</span></div></div></div></div></div><article class="single toc-start"><div class="content-block content-block-first content-block-position"><div class=post><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#debugpyの処理内容><code>debug.py</code>の処理内容</a><ul><li><a href=#概要>概要</a></li><li><a href=#詳細>詳細</a></li></ul></li></ul></nav></div></div><div style="margin:20px 0"><span class=post-update><b>Updated on 2020-04-28</b></span></div><p><a href=https://mshr-h.com/posts/onnx-mlir-debug-py/ rel>ONNX MLIRに付属のdebug.pyを動かす</a>で動かした<code>debug.py</code>の中を見る。</p><h2 id=debugpyの処理内容><code>debug.py</code>の処理内容</h2><h3 id=概要>概要</h3><ul><li>入力として指定したONNXモデルを、ONNX MLIRでshared libraryとしてビルド、実行し、リファレンスバックエンドで実行した結果と比較する</li><li>リファレンスバックエンドにはONNX Runtimeを使用</li><li>Operatorのoutputごとに比較<ul><li>モデルの出力だけでなく、Operatorの実行結果単位で比較している</li></ul></li><li><code>PyRuntime</code>はおそらく、ONNX MLIRでビルドしたshared libraryを、Pythonから実行するためのPythonバインディング<ul><li>shared libraryの実行方法は<code>PyRuntime</code>の実装を見る必要がありそう</li></ul></li></ul><h3 id=詳細>詳細</h3><p><code>import onnxruntime</code>でONNX Runtimeをインポートする。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>os</span>
<span class=kn>import</span> <span class=nn>sys</span>
<span class=kn>import</span> <span class=nn>argparse</span>
<span class=kn>import</span> <span class=nn>onnx</span>
<span class=kn>import</span> <span class=nn>subprocess</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=kn>as</span> <span class=nn>np</span>
<span class=kn>import</span> <span class=nn>tempfile</span>

<span class=kn>from</span> <span class=nn>collections</span> <span class=kn>import</span> <span class=n>OrderedDict</span>

<span class=c1># Reference backend, use onnxruntime by default</span>
<span class=kn>import</span> <span class=nn>onnxruntime</span>
<span class=n>prepare</span> <span class=o>=</span> <span class=n>onnxruntime</span><span class=o>.</span><span class=n>InferenceSession</span>
</code></pre></td></tr></table></div></div><p><code>ONNX_MLIR_HOME</code>が設定されているか確認。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=k>if</span> <span class=p>(</span><span class=ow>not</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;ONNX_MLIR_HOME&#39;</span><span class=p>,</span> <span class=bp>None</span><span class=p>)):</span>
    <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span>
        <span class=s2>&#34;Environment variable ONNX_MLIR_HOME is not set, please set it to the path to &#34;</span>
        <span class=s2>&#34;the HOME directory for onnx-mlir. The HOME directory for onnx-mlir refers to &#34;</span>
        <span class=s2>&#34;the parent folder containing the bin, lib, etc sub-folders in which ONNX-MLIR &#34;</span>
        <span class=s2>&#34;executables and libraries can be found.&#34;</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p><code>onnx-mlir</code>実行ファイルパス、<code>lib</code>ディレクトリをimport検索パスに追加などする。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=n>VERBOSE</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;VERBOSE&#39;</span><span class=p>,</span> <span class=bp>False</span><span class=p>)</span>
<span class=n>ONNX_MLIR</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;ONNX_MLIR_HOME&#39;</span><span class=p>],</span> <span class=s2>&#34;bin/onnx-mlir&#34;</span><span class=p>)</span>

<span class=c1># Include runtime directory in python paths, so PyRuntime can be imported.</span>
<span class=n>RUNTIME_DIR</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;ONNX_MLIR_HOME&#39;</span><span class=p>],</span> <span class=s2>&#34;lib&#34;</span><span class=p>)</span>
<span class=n>sys</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>RUNTIME_DIR</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p><code>PyRuntime</code>(ONNX MLIRでビルドしたshared libraryをPythonから実行するためのPythonバインディング？)をインポートする。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=k>try</span><span class=p>:</span>
    <span class=kn>from</span> <span class=nn>PyRuntime</span> <span class=kn>import</span> <span class=n>ExecutionSession</span>
<span class=k>except</span> <span class=ne>ImportError</span><span class=p>:</span>
    <span class=k>raise</span> <span class=ne>ImportError</span><span class=p>(</span>
        <span class=s2>&#34;Looks like you did not build the PyRuntime target, build it by running `make PyRuntime`.&#34;</span>
    <span class=p>)</span>
</code></pre></td></tr></table></div></div><p>ユーティリティ関数を定義。</p><p><code>extend_model_output</code>関数は、モデル内の各Operatorのoutputに、data type、shape情報を追加する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>execute_commands</span><span class=p>(</span><span class=n>cmds</span><span class=p>):</span>
    <span class=k>if</span> <span class=p>(</span><span class=n>VERBOSE</span><span class=p>):</span>
        <span class=k>print</span><span class=p>(</span><span class=s2>&#34; &#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>cmds</span><span class=p>))</span>
    <span class=n>subprocess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>cmds</span><span class=p>,</span> <span class=n>stdout</span><span class=o>=</span><span class=n>subprocess</span><span class=o>.</span><span class=n>PIPE</span><span class=p>,</span> <span class=n>check</span><span class=o>=</span><span class=bp>True</span><span class=p>)</span>


<span class=k>def</span> <span class=nf>extend_model_output</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>intermediate_outputs</span><span class=p>):</span>
    <span class=c1># onnx-mlir doesn&#39;t care about manually specified output types &amp; shapes.</span>
    <span class=n>DUMMY_TENSOR_TYPE</span> <span class=o>=</span> <span class=n>onnx</span><span class=o>.</span><span class=n>TensorProto</span><span class=o>.</span><span class=n>FLOAT</span>

    <span class=k>while</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>output</span><span class=p>)):</span>
        <span class=n>model</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>output</span><span class=o>.</span><span class=n>pop</span><span class=p>()</span>

    <span class=k>for</span> <span class=n>output_name</span> <span class=ow>in</span> <span class=n>intermediate_outputs</span><span class=p>:</span>
        <span class=n>output_value_info</span> <span class=o>=</span> <span class=n>onnx</span><span class=o>.</span><span class=n>helper</span><span class=o>.</span><span class=n>make_tensor_value_info</span><span class=p>(</span>
            <span class=n>output_name</span><span class=p>,</span> <span class=n>DUMMY_TENSOR_TYPE</span><span class=p>,</span> <span class=bp>None</span><span class=p>)</span>
        <span class=n>model</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>output</span><span class=o>.</span><span class=n>extend</span><span class=p>([</span><span class=n>output_value_info</span><span class=p>])</span>
    <span class=k>return</span> <span class=n>model</span>
</code></pre></td></tr></table></div></div><p>メイン関数。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>main</span><span class=p>(</span><span class=n>model_path</span><span class=p>):</span>
</code></pre></td></tr></table></div></div><p>入力のONNXファイルを読み込み、各Operatorのoutputにdata type、shape情報を追加する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>    <span class=n>model</span> <span class=o>=</span> <span class=n>onnx</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>model_path</span><span class=p>)</span>
    <span class=n>intermediate_outputs</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span>
        <span class=p>[</span><span class=nb>list</span><span class=p>(</span><span class=n>node</span><span class=o>.</span><span class=n>output</span><span class=p>)</span> <span class=k>for</span> <span class=n>node</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>node</span><span class=p>],</span> <span class=p>[])</span>
    <span class=n>intermediate_outputs</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>OrderedDict</span><span class=o>.</span><span class=n>fromkeys</span><span class=p>(</span><span class=n>intermediate_outputs</span><span class=p>))</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>extend_model_output</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>intermediate_outputs</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p>以下は一時ディレクトリ内で実行。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>    <span class=k>with</span> <span class=n>tempfile</span><span class=o>.</span><span class=n>TemporaryDirectory</span><span class=p>()</span> <span class=k>as</span> <span class=n>temp_dir</span><span class=p>:</span>
        <span class=k>print</span><span class=p>(</span><span class=s2>&#34;Temporary directory has been created at {}&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>temp_dir</span><span class=p>))</span>
</code></pre></td></tr></table></div></div><p>ONNXモデルをファイルに出力し、ONNX MLIRでshared libraryに変換する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>        <span class=c1># Save modified model &amp; invoke onnx-mlir to compile it.</span>
        <span class=n>temp_model_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>temp_dir</span><span class=p>,</span> <span class=s2>&#34;model.onnx&#34;</span><span class=p>)</span>
        <span class=n>onnx</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>temp_model_path</span><span class=p>)</span>
        <span class=n>execute_commands</span><span class=p>([</span><span class=n>ONNX_MLIR</span><span class=p>,</span> <span class=n>temp_model_path</span><span class=p>])</span>
</code></pre></td></tr></table></div></div><p>shared libraryから実行セッションを作成する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>        <span class=c1># Use the generated shared library to create an execution session.</span>
        <span class=n>temp_shared_lib_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>temp_dir</span><span class=p>,</span> <span class=s2>&#34;model.so&#34;</span><span class=p>)</span>
        <span class=n>sess</span> <span class=o>=</span> <span class=n>ExecutionSession</span><span class=p>(</span><span class=n>temp_shared_lib_path</span><span class=p>,</span>
                                <span class=s2>&#34;_dyn_entry_point_main_graph&#34;</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p>入力データとしてランダムデータを生成する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>        <span class=c1># Generate random data as input.</span>
        <span class=n>inputs</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=n>input_names</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=n>initializers</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=nb>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=o>.</span><span class=n>name</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>initializer</span><span class=p>))</span>
        <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
        <span class=k>for</span> <span class=n>input_proto</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>input</span><span class=p>:</span>
            <span class=k>if</span> <span class=n>input_proto</span><span class=o>.</span><span class=n>name</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>initializers</span><span class=p>:</span>
                <span class=n>input_names</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>input_proto</span><span class=o>.</span><span class=n>name</span><span class=p>)</span>
                <span class=n>shape_proto</span> <span class=o>=</span> <span class=n>input_proto</span><span class=o>.</span><span class=n>type</span><span class=o>.</span><span class=n>tensor_type</span><span class=o>.</span><span class=n>shape</span>
                <span class=n>explicit_shape</span> <span class=o>=</span> <span class=p>[]</span>
                <span class=k>for</span> <span class=n>dim</span> <span class=ow>in</span> <span class=n>shape_proto</span><span class=o>.</span><span class=n>dim</span><span class=p>:</span>
                    <span class=k>assert</span> <span class=n>dim</span><span class=o>.</span><span class=n>dim_value</span><span class=p>,</span> <span class=s2>&#34;Can only debug models with inputs that have explicit shapes.&#34;</span>
                    <span class=n>explicit_shape</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>dim</span><span class=o>.</span><span class=n>dim_value</span><span class=p>)</span>
                <span class=n>inputs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
                    <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=o>-</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=n>explicit_shape</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>))</span>
</code></pre></td></tr></table></div></div><p>shared libraryの実行セッションを実行する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>        <span class=c1># Run the compiled inference function on the randomly generated data.</span>
        <span class=n>outs</span> <span class=o>=</span> <span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p>リファレンスバックエンド(ONNX Runtime)で実行する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>        <span class=c1># Run the model with reference backend and get results.</span>
        <span class=n>ref_session</span> <span class=o>=</span> <span class=n>prepare</span><span class=p>(</span><span class=n>temp_model_path</span><span class=p>)</span>
        <span class=n>output_names</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=nb>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=o>.</span><span class=n>name</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>output</span><span class=p>))</span>
        <span class=n>input_feed</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>input_names</span><span class=p>,</span> <span class=n>inputs</span><span class=p>))</span>
        <span class=n>ref_outs</span> <span class=o>=</span> <span class=n>ref_session</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>output_names</span><span class=p>,</span> <span class=n>input_feed</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p>shared libraryとONNX Runtimeで実行した結果を比較する。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python>        <span class=c1># For each intermediate output tensor, compare results.</span>
        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>name</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>intermediate_outputs</span><span class=p>):</span>
            <span class=k>print</span><span class=p>(</span><span class=s2>&#34;Verifying value of {}&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>name</span><span class=p>))</span>
            <span class=n>np</span><span class=o>.</span><span class=n>testing</span><span class=o>.</span><span class=n>assert_array_almost_equal</span><span class=p>(</span><span class=n>ref_outs</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>outs</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>decimal</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p>引数パーサーを定義し、メイン関数へパース結果を渡す。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
    <span class=n>parser</span> <span class=o>=</span> <span class=n>argparse</span><span class=o>.</span><span class=n>ArgumentParser</span><span class=p>()</span>
    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s1>&#39;model_path&#39;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>str</span><span class=p>,</span> <span class=n>help</span><span class=o>=</span><span class=s2>&#34;Path to the model to debug.&#34;</span><span class=p>)</span>
    <span class=n>args</span> <span class=o>=</span> <span class=n>parser</span><span class=o>.</span><span class=n>parse_args</span><span class=p>()</span>
    <span class=n>main</span><span class=p>(</span><span class=o>**</span><span class=nb>vars</span><span class=p>(</span><span class=n>args</span><span class=p>))</span>
</code></pre></td></tr></table></div></div></div><div class=post style=padding-top:0><div class=post-share><div class=share-link><a class="share-icon share-twitter" href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://mshr-h.com/posts/onnx-mlir-detail-debug-py/ data-title="ONNX MLIRのdebug.py詳細" data-via=mshrh3 data-hashtags=ONNX,MLIR,Python><span class="svg-social-icon icon-twitter"></span></a></div><div class=share-link><a class="share-icon share-hackernews" href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://mshr-h.com/posts/onnx-mlir-detail-debug-py/ data-title="ONNX MLIRのdebug.py詳細"><span class="svg-social-icon icon-hacker-news"></span></a></div></div><div class=post-tags><a href=/tags/onnx/ class=tag>ONNX</a><a href=/tags/mlir/ class=tag>MLIR</a><a href=/tags/python/ class=tag>Python</a></div></div></div><div id=toc-final></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.82.0">Hugo</a> | Theme - <a href="https://ublogger.netlify.app/?utm_source=https://mshr-h.com&utm_medium=footer&utm_campaign=config&utm_term=2.0.1" target=_blank title="uBlogger 2.0.1">uBlogger</a></div><div class=footer-line><i class="svg-icon icon-copyright"></i><span>2018 - 2021</span><span class=author>&nbsp;<a href=https://mshr-h.com target=_blank>Masa</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="svg-icon icon-arrow-up"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="svg-icon icon-comments-fixed"></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.css><link rel=stylesheet href=/custom.css><script src=https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js></script><script src=https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js></script><script src=https://cdn.jsdelivr.net/npm/sharer.js@0.4.0/sharer.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.min.js></script><script>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:15},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1}}</script><script src=/js/theme.min.js></script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-165309045-1','auto'),ga('set','anonymizeIp',!0),ga('send','pageview')</script><script>(function(a,e,f,g,b,c,d){a[b]=a[b]||function(){(a[b].a=a[b].a||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,"script","https://mc.yandex.ru/metrika/tag.js","ym"),ym("70532758","init",{clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:null})</script><noscript><div><img src=https://mc.yandex.ru/watch/70532758 style=position:absolute;left:-9999px alt></div></noscript></body></html>